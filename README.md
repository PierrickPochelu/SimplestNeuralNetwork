This example is a very simple multi layer perceptron that learns with SGD.
For the learning part:
* SGD : https://en.wikipedia.org/wiki/Stochastic_gradient_descent

For the neural architecture:
* https://en.wikipedia.org/wiki/Universal_approximation_theorem

A lot of implementation on the internet presents errors, for example forgetting the bias in formula. It can helps student to understand computing, specially back propagation which is not always intuitive.
